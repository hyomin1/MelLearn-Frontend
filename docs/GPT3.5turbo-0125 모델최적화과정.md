# gpt3.5-turbo-0125 모델 성능 최적화 과정

## 만들어야 하는 모델
노래의 가사를 주면 사용자의 학습능력에 맞는 10개의 문제를 제공하는 인공지능 모델

## Playground에서 사용해보며 알게된 것.

파인튜닝 전에 모델을 사용해보며 범위를 알아야 한다. 파인튜닝으로 얻고싶은 결과의 범위를 정해야한다.
Playground에서 System에 역할을 부여한 후 gpt-3.5turbo-0125 모델에 문제 생성을 요청해보았다.
System에 부여한 역할은 아래와 같다.

> 너는 외국어 단어 문제 출제 위원이야.
> 너는 유저가 준 인풋들을 토대로 문제집을 만들어야해.
> 너가 만든 문제의 선택지와 정답은 애매하지 않고 반드시 명확해야해.
> 아래는 유저 인풋 설명이야.
> 1. level : 1~3까지의 범위가 주어져. 3단계로 갈수록 전문가 수준의 문제를 출제해야해.
> 2. totalProblem : 너가 만들어야 하는 문제의 개수를 나타내.
> 3. text: 유저가 제공하는 문장들이야. 
> 아래는 assistence 아웃풋 설명이야.
> 1. question : 너는 문제 출제 방식으로 빈칸뚫기, 동의어 반의어를 물어보는 유형을 채택해야해. 한 문장에 두개 이상의 question을 물어볼 수 없어. 'text'에서 인용한 외국어를 제외하고는 한국어를 사용해서 질문해야해.
> 2. selection : 4가지 선택지를 제공해야해. 각 선택지에 있는 정답과 오답이 명확해야해. 선택지는 외국어로 제공되어야 해.
> 3. answer : 선택지중에 정답인 선택지를 정수로 알려줘야해.
> 4. comment : 문제가 정답인 이유를 자세하게 적어줘야해. 유저가 알아 들을 수 있도록 반드시 한국어로 설명해야해.

### 1. 필요없는 문장들 제거. 

2번째 줄. 너는 유저가 준 인풋들을 토대로 문제집을 만들어야해. -> 이 설명은 너무 당연하다. 따라서 필요없는 문장이다.
3번째 줄. 너가 만든 문제의 선택지와 정답은 애매하지 않고 반드시 명확해야해. -> 어떤 문제와 선택지가 애매한지 설명하지 않았다. 필요없는 문장인거 같다.

따라서 2,3번째 줄은 삭제하는게 맞다. 토큰만 잡아먹는다.

### 2. 명확한 명령을 하지 않았다.

level을 1~3까지 나누었는데 3단계로 갈수록 전문가 수준이라고 모델에게 설명했다. 
하지만 1단계에 대한 설명을 하지 않고 최대 단계의 수준만 모델에게 설명했기 때문에 모델이 제공하는 1~3단계의 문제 수준 차이가 모호하다고 느꼈다.
그래서 1단계에 대해서 해당 언어를 처음 배우는 초보자 라는 설명을 추가했다.

### 3. 응답 포맷을 system의 역할부여에 적을필요가 없다.

응답의 포맷은 파인튜닝과 Function Calling을 사용하여 해결할 수 있다.
따라서 System에 모델의 응답 포맷을 적지 않아도 된다.
특히 Function Calling을 사용해서 우리 모델의 포맷에 맞춰주는 작업을 수행할 수 있는 것 같다. 이건 더 알아보자.

## 4. 발생한 문제와 해결방법 생각.

### 1. 문제, 선택지, 해설의 사용 언어.

문제와 해설은 사용자의 언어(한국어)로 제공되어야 한다. 이 때 본문에서 인용한 글은 그 언어 그대로 제공되어야 한다.
예를들어
> 문장에서 'We will be alright in the afterlife' 'alright'의 반의어는 무엇인가?
> 라는 문제를 기대하여 프롬프트를 작성했지만, 다음과 같은 문제가 발생했다.
>> 1. 인용한 영어본문을 한국어로 해석하여 문제를 출제
>> 2. 문제를 영어로 출제하여 영어에 익숙하지 않은 사용자가 문제의 난이도보다 문제 해석이 어려워지는 문제 발생
>> 3. 선택지, 해설의 언어가 매 요청마다 달라지는 문제

종합하자면 매 요청마다 문제의 형식이 달라지는 문제가 발생했다.
이 문제는 프롬프트 엔지니어링으로 해결될 수 있는 문제라고 보여진다. 하지만 계속 프롬프트를 변경해봤지만 획일화된 답변을 얻을 수 없었다.
다시 시도해보고 만약 안될경우 파인튜닝으로 해결할 생각을 해야겠다.
파인튜닝 데이터셋을 준비할 때 모든 문제, 선택지, 해설의 입력 형식을 일치시키면 해결될 것 같다.

### 2. 최대 토큰수와 과적합 문제

파인튜닝 시킬 수 있는 토큰 수가 16,385토큰으로 정해져 있다.
파인튜닝을 시킬 때 최소 10개의 데이터셋을 준비해야 한다.
노래의 가사를 받아 10개의 문제와 해설을 제공하는 예제를 10개 만들어야 된다는 얘기다.
노래의 가사도 길고, 문제와 해설 10개도 많아 최대 토큰 수를 넘어버리는 문제가 발생하였다.
따라서 totalProblem key를 유저 input으로 받아서 학습 데이터셋의 totalProblem을 1로 고정시켜 토큰 수를 줄이고, 나중에 호출할 때 10으로 호출시킬 생각으로 파인튜닝 모델을 만들었다.
하지만 학습 데이터에서 1개의 문제를 만들어주는 assistence의 아웃풋을 학습했기 때문에 10개를 만들어주라고 호출해도 1개의 문제만 만들어주는 오류가 발생했다.
학습 데이터의 토큰수는 줄이고, 과적합도 해결하는 데이터셋을 새로 짜야한다.
요청하는 문제 수를 한문제로 고정하지말고, 1~3문제를 번갈아가며 학습 데이터셋을 만들어야 한다. 다른 요인들도 마찬가지.

